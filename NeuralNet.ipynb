{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt # to visualize only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv(\"./data/processed_x.csv\", delimiter=\",\", header = None) # load from processed images\n",
    "# train_x = train_x.values # dataframe to numpy ndarray\n",
    "# train_x = train_x.astype(np.float32)\n",
    "# train_x /= 255 # normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"./data/train_y.csv\", delimiter = \",\", header = None)\n",
    "# train_y = (train_y.values).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9ad37dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at some data for sanity check\n",
    "def plotter(images, labels, start):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    for index, (image, label) in enumerate(zip(images[start:start+5], labels[start:start+5])):\n",
    "        plt.subplot(1, 5, index + 1)\n",
    "        plt.imshow(np.reshape(image, (64,64)), cmap= 'gray')\n",
    "        plt.title('Training: %i\\n' % label, fontsize = 20)\n",
    "plotter(train_x,train_y, 0) # first 5 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            valid_data):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if valid_data: n_valid = len(valid_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "#             mini_batches = [\n",
    "#                 training_data[k:k+mini_batch_size]\n",
    "#                 for k in xrange(0, n, mini_batch_size)]\n",
    "#             for mini_batch in mini_batches:\n",
    "            self.update_mini_batch(training_data, eta)\n",
    "            \n",
    "            valid_score = self.evaluate(valid_data)\n",
    "            print \"Epoch {0}: {1} / {2}\".format(\n",
    "                j, valid_score, n_valid)\n",
    "        \n",
    "        return valid_score\n",
    "        \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "#         self.weights = [w-(eta)*nw\n",
    "#                         for w, nw in zip(self.weights, delta_nabla_w)]\n",
    "#         self.biases = [b-(eta)*nb\n",
    "#                        for b, nb in zip(self.biases, delta_nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        \n",
    "#         for x, y in test_data:\n",
    "#             print self.feedforward(x)\n",
    "#             print y\n",
    "            \n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 22 / 200\n",
      "Epoch 1: 23 / 200\n",
      "Epoch 2: 29 / 200\n",
      "Epoch 3: 22 / 200\n",
      "Epoch 4: 24 / 200\n",
      "Epoch 5: 24 / 200\n",
      "Epoch 6: 24 / 200\n",
      "Epoch 7: 25 / 200\n",
      "Epoch 8: 27 / 200\n",
      "Epoch 9: 27 / 200\n",
      "Epoch 10: 27 / 200\n",
      "Epoch 11: 26 / 200\n",
      "Epoch 12: 27 / 200\n",
      "Epoch 13: 27 / 200\n",
      "Epoch 14: 29 / 200\n",
      "Epoch 15: 29 / 200\n",
      "Epoch 16: 29 / 200\n",
      "Epoch 17: 29 / 200\n",
      "Epoch 18: 30 / 200\n",
      "Epoch 19: 30 / 200\n",
      "Epoch 20: 30 / 200\n",
      "Epoch 21: 30 / 200\n",
      "Epoch 22: 29 / 200\n",
      "Epoch 23: 30 / 200\n",
      "Epoch 24: 30 / 200\n",
      "Epoch 25: 29 / 200\n",
      "Epoch 26: 28 / 200\n",
      "Epoch 27: 31 / 200\n",
      "Epoch 28: 32 / 200\n",
      "Epoch 29: 32 / 200\n",
      "Epoch 30: 31 / 200\n",
      "Epoch 31: 32 / 200\n",
      "Epoch 32: 33 / 200\n",
      "Epoch 33: 34 / 200\n",
      "Epoch 34: 34 / 200\n",
      "Epoch 35: 35 / 200\n",
      "Epoch 36: 33 / 200\n",
      "Epoch 37: 35 / 200\n",
      "Epoch 38: 35 / 200\n",
      "Epoch 39: 34 / 200\n",
      "Epoch 40: 34 / 200\n",
      "Epoch 41: 35 / 200\n",
      "Epoch 42: 36 / 200\n",
      "Epoch 43: 37 / 200\n",
      "Epoch 44: 36 / 200\n",
      "Epoch 45: 36 / 200\n",
      "Epoch 46: 37 / 200\n",
      "Epoch 47: 37 / 200\n",
      "Epoch 48: 36 / 200\n",
      "Epoch 49: 36 / 200\n",
      "Epoch 50: 36 / 200\n",
      "Epoch 51: 36 / 200\n",
      "Epoch 52: 36 / 200\n",
      "Epoch 53: 36 / 200\n",
      "Epoch 54: 36 / 200\n",
      "Epoch 55: 36 / 200\n",
      "Epoch 56: 36 / 200\n",
      "Epoch 57: 36 / 200\n",
      "Epoch 58: 36 / 200\n",
      "Epoch 59: 37 / 200\n",
      "Epoch 60: 37 / 200\n",
      "Epoch 61: 37 / 200\n",
      "Epoch 62: 38 / 200\n",
      "Epoch 63: 38 / 200\n",
      "Epoch 64: 38 / 200\n",
      "Epoch 65: 39 / 200\n",
      "Epoch 66: 40 / 200\n",
      "Epoch 67: 40 / 200\n",
      "Epoch 68: 40 / 200\n",
      "Epoch 69: 40 / 200\n",
      "Epoch 70: 40 / 200\n",
      "Epoch 71: 40 / 200\n"
     ]
    }
   ],
   "source": [
    "data_x = train_x.head(1000)\n",
    "data_y = train_y.head(1000)\n",
    "\n",
    "data_x.reset_index\n",
    "data_y.reset_index\n",
    "\n",
    "data = pd.concat([data_x, data_y], axis=1)\n",
    "data.columns = [i for i in range(data.shape[1])]\n",
    "\n",
    "# test_x = train_x.tail(100)\n",
    "# test_y = train_y.tail(100)\n",
    "\n",
    "fifth = len(data.index)/5\n",
    "\n",
    "sets  = []\n",
    "sets.append([data.head(fifth).reset_index(), data.tail(len(data)-fifth).reset_index()])\n",
    "sets.append([data.iloc[fifth:fifth*2].reset_index(), data.tail(len(data)-fifth*2).append(data.head(fifth)).reset_index()])\n",
    "sets.append([data.iloc[fifth*2:fifth*3].reset_index(), data.tail(fifth*2).append(data.head(fifth*2+4)).reset_index()])\n",
    "sets.append([data.iloc[fifth*3:fifth*4].reset_index(), data.tail(fifth+4).append(data.head(fifth*3)).reset_index()])\n",
    "sets.append([data.tail(fifth).reset_index(), data.head(fifth*4+4).reset_index()])\n",
    "\n",
    "average_accuracy = 0\n",
    "for i in range(5):\n",
    "    # Training and validation set of chunk i\n",
    "    trainX = sets[i][1][sets[i][1].columns[:-1]].drop(['index'], axis=1)\n",
    "    trainY = sets[i][1][sets[i][1].columns[-1:]]\n",
    "    trainY.columns = [0]\n",
    "    \n",
    "    validX = sets[i][0][sets[i][0].columns[:-1]].drop(['index'], axis=1)\n",
    "    validY = sets[i][0][sets[i][0].columns[-1:]]\n",
    "    validY.columns = [0]\n",
    "    \n",
    "    training_y = map(int, trainY[0].tolist())\n",
    "    valid_y = map(int, validY[0].tolist())\n",
    "\n",
    "    for i in range(len(training_y)):\n",
    "        temp = np.zeros((10,1))\n",
    "        temp[training_y[i]] = 1\n",
    "        training_y[i] = temp\n",
    "    \n",
    "    training_x = trainX.values.astype(np.float32)\n",
    "    training_x /= 255\n",
    "\n",
    "    valid_x = validX.values.astype(np.float32)\n",
    "    valid_x /= 255\n",
    "\n",
    "    training_inputs = [np.reshape(x,(training_x.shape[1],1)) \n",
    "                       for x in training_x]\n",
    "    valid_inputs = [np.reshape(x,(valid_x.shape[1],1)) for x in valid_x]\n",
    "    training_data = zip(training_inputs, training_y)\n",
    "    valid_data = zip(valid_inputs, valid_y)\n",
    "    \n",
    "    neural_net = Network([4096, 100, 10])\n",
    "    \n",
    "    valid_score = neural_net.SGD(training_data, 100, 4096, 3.0, valid_data)\n",
    "    average_accuracy += valid_score\n",
    "\n",
    "print average_accuracy / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8 / 100\n",
      "Epoch 1: 9 / 100\n",
      "Epoch 2: 10 / 100\n",
      "Epoch 3: 10 / 100\n",
      "Epoch 4: 10 / 100\n",
      "Epoch 5: 10 / 100\n",
      "Epoch 6: 10 / 100\n",
      "Epoch 7: 7 / 100\n",
      "Epoch 8: 6 / 100\n",
      "Epoch 9: 7 / 100\n",
      "Epoch 10: 8 / 100\n",
      "Epoch 11: 9 / 100\n",
      "Epoch 12: 10 / 100\n",
      "Epoch 13: 11 / 100\n",
      "Epoch 14: 11 / 100\n",
      "Epoch 15: 11 / 100\n",
      "Epoch 16: 11 / 100\n",
      "Epoch 17: 11 / 100\n",
      "Epoch 18: 11 / 100\n",
      "Epoch 19: 11 / 100\n",
      "Epoch 20: 11 / 100\n",
      "Epoch 21: 11 / 100\n",
      "Epoch 22: 11 / 100\n",
      "Epoch 23: 11 / 100\n",
      "Epoch 24: 11 / 100\n",
      "Epoch 25: 11 / 100\n",
      "Epoch 26: 11 / 100\n",
      "Epoch 27: 11 / 100\n",
      "Epoch 28: 11 / 100\n",
      "Epoch 29: 11 / 100\n",
      "Epoch 30: 11 / 100\n",
      "Epoch 31: 11 / 100\n",
      "Epoch 32: 11 / 100\n",
      "Epoch 33: 11 / 100\n",
      "Epoch 34: 11 / 100\n",
      "Epoch 35: 11 / 100\n",
      "Epoch 36: 11 / 100\n",
      "Epoch 37: 11 / 100\n",
      "Epoch 38: 11 / 100\n",
      "Epoch 39: 11 / 100\n",
      "Epoch 40: 12 / 100\n",
      "Epoch 41: 12 / 100\n",
      "Epoch 42: 12 / 100\n",
      "Epoch 43: 12 / 100\n",
      "Epoch 44: 12 / 100\n",
      "Epoch 45: 12 / 100\n",
      "Epoch 46: 12 / 100\n",
      "Epoch 47: 12 / 100\n",
      "Epoch 48: 12 / 100\n",
      "Epoch 49: 12 / 100\n",
      "Epoch 50: 12 / 100\n",
      "Epoch 51: 12 / 100\n",
      "Epoch 52: 12 / 100\n",
      "Epoch 53: 13 / 100\n",
      "Epoch 54: 13 / 100\n",
      "Epoch 55: 13 / 100\n",
      "Epoch 56: 13 / 100\n",
      "Epoch 57: 13 / 100\n",
      "Epoch 58: 13 / 100\n",
      "Epoch 59: 13 / 100\n",
      "Epoch 60: 13 / 100\n",
      "Epoch 61: 13 / 100\n",
      "Epoch 62: 13 / 100\n",
      "Epoch 63: 12 / 100\n",
      "Epoch 64: 13 / 100\n",
      "Epoch 65: 13 / 100\n",
      "Epoch 66: 13 / 100\n",
      "Epoch 67: 13 / 100\n",
      "Epoch 68: 13 / 100\n",
      "Epoch 69: 13 / 100\n",
      "Epoch 70: 13 / 100\n",
      "Epoch 71: 13 / 100\n",
      "Epoch 72: 13 / 100\n",
      "Epoch 73: 13 / 100\n",
      "Epoch 74: 13 / 100\n",
      "Epoch 75: 13 / 100\n",
      "Epoch 76: 13 / 100\n",
      "Epoch 77: 13 / 100\n",
      "Epoch 78: 13 / 100\n",
      "Epoch 79: 13 / 100\n",
      "Epoch 80: 13 / 100\n",
      "Epoch 81: 13 / 100\n",
      "Epoch 82: 13 / 100\n",
      "Epoch 83: 13 / 100\n",
      "Epoch 84: 13 / 100\n",
      "Epoch 85: 13 / 100\n",
      "Epoch 86: 13 / 100\n",
      "Epoch 87: 13 / 100\n",
      "Epoch 88: 13 / 100\n",
      "Epoch 89: 13 / 100\n",
      "Epoch 90: 13 / 100\n",
      "Epoch 91: 13 / 100\n",
      "Epoch 92: 13 / 100\n",
      "Epoch 93: 13 / 100\n",
      "Epoch 94: 13 / 100\n",
      "Epoch 95: 13 / 100\n",
      "Epoch 96: 13 / 100\n",
      "Epoch 97: 13 / 100\n",
      "Epoch 98: 13 / 100\n",
      "Epoch 99: 13 / 100\n",
      "Epoch 100: 13 / 100\n",
      "Epoch 101: 13 / 100\n",
      "Epoch 102: 13 / 100\n",
      "Epoch 103: 13 / 100\n",
      "Epoch 104: 13 / 100\n",
      "Epoch 105: 13 / 100\n",
      "Epoch 106: 12 / 100\n",
      "Epoch 107: 12 / 100\n",
      "Epoch 108: 11 / 100\n",
      "Epoch 109: 11 / 100\n",
      "Epoch 110: 11 / 100\n",
      "Epoch 111: 11 / 100\n",
      "Epoch 112: 11 / 100\n",
      "Epoch 113: 11 / 100\n",
      "Epoch 114: 11 / 100\n",
      "Epoch 115: 11 / 100\n",
      "Epoch 116: 11 / 100\n",
      "Epoch 117: 11 / 100\n",
      "Epoch 118: 11 / 100\n",
      "Epoch 119: 11 / 100\n",
      "Epoch 120: 11 / 100\n",
      "Epoch 121: 11 / 100\n",
      "Epoch 122: 11 / 100\n",
      "Epoch 123: 11 / 100\n",
      "Epoch 124: 11 / 100\n",
      "Epoch 125: 11 / 100\n",
      "Epoch 126: 11 / 100\n",
      "Epoch 127: 11 / 100\n",
      "Epoch 128: 11 / 100\n",
      "Epoch 129: 11 / 100\n",
      "Epoch 130: 11 / 100\n",
      "Epoch 131: 11 / 100\n",
      "Epoch 132: 11 / 100\n",
      "Epoch 133: 11 / 100\n",
      "Epoch 134: 12 / 100\n",
      "Epoch 135: 12 / 100\n",
      "Epoch 136: 12 / 100\n",
      "Epoch 137: 12 / 100\n",
      "Epoch 138: 12 / 100\n",
      "Epoch 139: 12 / 100\n",
      "Epoch 140: 12 / 100\n",
      "Epoch 141: 12 / 100\n",
      "Epoch 142: 12 / 100\n",
      "Epoch 143: 12 / 100\n",
      "Epoch 144: 12 / 100\n",
      "Epoch 145: 12 / 100\n",
      "Epoch 146: 12 / 100\n",
      "Epoch 147: 12 / 100\n",
      "Epoch 148: 12 / 100\n",
      "Epoch 149: 12 / 100\n",
      "Epoch 150: 12 / 100\n",
      "Epoch 151: 12 / 100\n",
      "Epoch 152: 12 / 100\n",
      "Epoch 153: 13 / 100\n",
      "Epoch 154: 13 / 100\n",
      "Epoch 155: 13 / 100\n",
      "Epoch 156: 13 / 100\n",
      "Epoch 157: 13 / 100\n",
      "Epoch 158: 13 / 100\n",
      "Epoch 159: 13 / 100\n",
      "Epoch 160: 13 / 100\n",
      "Epoch 161: 13 / 100\n",
      "Epoch 162: 13 / 100\n",
      "Epoch 163: 13 / 100\n",
      "Epoch 164: 13 / 100\n",
      "Epoch 165: 13 / 100\n",
      "Epoch 166: 13 / 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-935587cb7664>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneural_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-47a4a8e67c51>\u001b[0m in \u001b[0;36mSGD\u001b[1;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m#                 for k in xrange(0, n, mini_batch_size)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#             for mini_batch in mini_batches:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 print \"Epoch {0}: {1} / {2}\".format(\n",
      "\u001b[1;32m<ipython-input-19-47a4a8e67c51>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[1;34m(self, mini_batch, eta)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mnabla_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[0mnabla_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdnb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_nabla_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mnabla_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdnw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnabla_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-47a4a8e67c51>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mnabla_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mnabla_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnabla_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print neural_net.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
